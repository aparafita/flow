{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial deals with how to implement your flow using the tools provided by this library. We'll start the explanation talking about general flows, since this allows us to explain the architecture for Transformers and for Conditioners.\n",
    "\n",
    "* [Flow](#flow)\n",
    "* [Transformer](#transformer)\n",
    "* [Conditioner](#conditioner)\n",
    "    * [Conditional Conditioners](#cond-conditioners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"flow\" />\n",
    "\n",
    "# Implementing my own Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing a Flow from scratch, three functions need to be overriden in the inheriting class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow import Flow\n",
    "\n",
    "class MyOwnFlow(Flow):\n",
    "    \n",
    "    def _transform(self, x, log_det=False, **kwargs): \n",
    "        # Transforms x into u. Used for training.\n",
    "        pass\n",
    "\n",
    "    def _invert(self, u, log_det=False, **kwargs): \n",
    "        # Transforms u into x. Used for sampling.\n",
    "        pass\n",
    "\n",
    "    def warm_start(self, x, **kwargs):\n",
    "        # Warm start operation for the flow, if necessary.\n",
    "        return self # don't forget to return self!\n",
    "    \n",
    "    # Also, if you need to save any parameters or buffers, for the init:\n",
    "    def __init__(self, arg1, arg2, **kwargs):\n",
    "        # Don't forget to capture **kwargs, that will need to be passed to the base class\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Here we do what we want with the initializing function\n",
    "        self.arg1 = arg1\n",
    "        self.arg2 = arg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two important aspects to take into account. **Always** capture `**kwargs` in each of these functions, since there may be other flows that depend on this (for example, conditioning tensors).\n",
    "\n",
    "On the other hand, notice that `log_det=False` argument? If True, both `_transform` and `_invert` need to compute the logarithm of the absolute value of the determinant of the Jacobian of the transformation at x or u, respectively. This term is used in the computation of the nll of a sample, and it's essential for training. \n",
    "\n",
    "If `log_det` is True, these functions are expected to compute **and** return that log det. In that case, the return type is a tuple with the transformed tensor and the log det, like this: `return u, log_det`. If `log_det` is False, \n",
    "\n",
    "Taking this into account, you can do whatever you want with this Flow. Look for the implementations in the library to see some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transformer\" />\n",
    "\n",
    "# Implementing my own Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers are a special kind of Flow. When we call them directly (through `__call__` and eventually `forward`) we give them an additional positional argument, h. h are the parameters that this transformer needs. For example, in an Affine transformer, h are loc and scale. \n",
    "\n",
    "When we define a Transformer, we need to implement the following methods in the inheriting class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow import Transformer\n",
    "\n",
    "class MyOwnAffineTransformer(Transformer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        # Extend the constructor to pass, to its base class,\n",
    "        # the required kwarg h_dim >= 0. \n",
    "        \n",
    "        # h_dim is the number of dimensions that your parameters (in total) require PER DIMENSION.\n",
    "        # For example, in an Affine transformer, we have 2 parameters, loc and scale, \n",
    "        # each of dimension 1 per each dimension we want to transform. That means,\n",
    "        # h_dim = 1 (for scale) + 1 (for loc). If the Flow is used with a 10-dimensional distribution,\n",
    "        # then we'll have 10 pairs of parameters, each of h_dim=2 dimensions \n",
    "        # (the first one for scale, the second one for loc).\n",
    "        \n",
    "        # When we define h_dim, we pass it to the base class:\n",
    "        h_dim = 2 # whatever you need here\n",
    "        super().__init__(h_dim=h_dim, **kwargs) # call the base class\n",
    "        \n",
    "        # do something else if you need to\n",
    "        # ...\n",
    "\n",
    "    def _activation(self, h, **kwargs): \n",
    "        # Transform h by activation before calling _transform or _invert.\n",
    "        # \n",
    "        # For example, for a scale parameter, _activation could pass h\n",
    "        # through a softplus function to make it positive.\n",
    "        # Returns a tuple with the activated tensor parameters.\n",
    "        \n",
    "        # Let's do the Affine one\n",
    "        loc, log_scale = h[:, ::2], h[:, 1::2]\n",
    "        # here, the odd positions are for loc, and the even ones for log_scale\n",
    "        # Note that h can contain a single dimension's worth of parameters,\n",
    "        # or all dimensions at the same time. That's why we use this approach here,\n",
    "        # getting all odds or evens at once.\n",
    "        \n",
    "        # dimension parameters always come together, like this:\n",
    "        # loc1 | log_scale1 | loc2 | log_scale2\n",
    "        # Always take this into account.\n",
    "        # Otherwise, some Conditioners might not work with your transformer.\n",
    "        \n",
    "        # Now, we need to pass an activation function through log_scale to transform it\n",
    "        # into a positive tensor.\n",
    "        scale = torch.exp(log_scale)\n",
    "        \n",
    "        return loc, scale # notice how we return all divided parameters in a single tuple\n",
    "        # even if you return a single parameter, return it as a tuple, like this:\n",
    "        # return loc,\n",
    "        \n",
    "    def _transform(self, x, *h, log_det=False, **kwargs): \n",
    "        # Transform x into u using parameters h.\n",
    "        # Notice that, in contrast with a base Flow, here we receive our tuple of parameters\n",
    "        # as positional arguments. Here h is the tuple we return from _activation\n",
    "        loc, scale = h # unpack the tuple\n",
    "        \n",
    "        u = x * scale + loc\n",
    "        \n",
    "        if log_det: # remember to return log_det if required\n",
    "            # the determinant for this function is the sum of the diagonal's elements\n",
    "            log_det = torch.log(scale).sum(dim=1)\n",
    "            \n",
    "            return u, log_det\n",
    "        else:\n",
    "            return u\n",
    "\n",
    "    def _invert(self, u, *h, log_det=False, **kwargs):\n",
    "        # Transform u into x using parameters h.\n",
    "        loc, scale = h # unpack again\n",
    "        \n",
    "        x = (u - loc) / scale\n",
    "        \n",
    "        if log_det:\n",
    "            log_det = -torch.log(scale).sum(dim=1) # log_det is always the opposite in invert\n",
    "            \n",
    "            return x, log_det\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def _h_init(self):\n",
    "        # Return initialization values for pre-activation h parameters.\n",
    "        \n",
    "        # If you want to set the pre-_activation parameters h to a default, stable value,\n",
    "        # return that value in this function.\n",
    "        # This is useful to initialize your flow to perform the identity function at first.\n",
    "        # This helps in stabilizing training.\n",
    "        \n",
    "        # For this example, to make it the identity function, we need scale = 1 and loc = 0.\n",
    "        # But remember, we return the pre-_activation values! The inverse of exp is log, and log(1) is 0.\n",
    "        # So we just need to return 0s\n",
    "        h_init = torch.zeros(self.dim * self.h_dim, device=self.device)\n",
    "        # Notice that the shape of h_init is (self.dim * self.h_dim),\n",
    "        # which means, h_dim dimensions per each distribution's dimension.\n",
    "\n",
    "        return h_init\n",
    "        \n",
    "        # If you don't want to do initialization, just return None\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conditioner\" />\n",
    "\n",
    "# Implementing my own Conditioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditioners use the abstract interface we described before to define a Flow irrespective of the actual transformation we perform with its Transformer. \n",
    "\n",
    "When we define a Conditioner, we need to implement the following methods in the inheriting class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow import Conditioner\n",
    "\n",
    "class MyOwnConditioner(Conditioner):\n",
    "    \n",
    "    conditional = False # True means that this is a conditional-enabled Conditioner\n",
    "    \n",
    "    # Take into account that a Conditioner has an attribute self.trnf\n",
    "    # that contains the transformer it's going to use.\n",
    "    \n",
    "    def _h(self, x, cond=None, **kwargs): \n",
    "        # Return the (non-activated) tensor of parameters h \n",
    "        # corresponding to the given x. If this is a conditional flow,\n",
    "        # the conditioning tensor is passed as the 'cond' kwarg.\n",
    "        \n",
    "        # For example, this calls a network that gets x as input and returns h.\n",
    "        pass\n",
    "\n",
    "    def _invert(self, u, cond=None, log_det=False, **kwargs): \n",
    "        # Transform u into x.\n",
    "        pass\n",
    "        \n",
    "    # Also, optionally, you might want to use _h_init in your __init__\n",
    "    # to initialize your network. Look at flow.conditioner for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a Conditioner does not implement a `_transform` method. This is taken care of by the Conditioner abstract class directly. What it does is call `_h` with the given x (and maybe also cond) and obtains the resulting h pre-activation parameters. Then, it passes x and h to the Transformer's `forward` method, which is the one that actually performs the transformation.\n",
    "\n",
    "However, you do need to define the `_invert` operation. Since each Conditioner is different, you need to specify how exactly your Conditioner is going to invert u into x. The rules for `log_det` are the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cond-conditioners\" />\n",
    "\n",
    "# Conditional Conditioners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to take into account with Conditioners is if you wanna make them conditional or not. The library expects you to support conditional distributions (see the Iris tutorial). If you do not want to support it, mark your Conditioner class with conditional=False as above.\n",
    "\n",
    "If you do want to support it, make sure you use the `cond` keyword argument in both `_h` and `_invert`. `cond` contains the conditioning tensor, that is expected to have `self.cond_dim` dimensions. If `self.cond_dim` is 0, it means that the used is not using your Conditioner in conditional-mode, and, as such, cond should be ignored (will be None). Look at the examples in `flow.conditioner` for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
