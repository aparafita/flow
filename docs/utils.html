<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>flow.utils API documentation</title>
<meta name="description" content="Miscellaneous utility functions." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML'></script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flow.utils</code></h1>
</header>
<section id="section-intro">
<p>Miscellaneous utility functions.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Miscellaneous utility functions.
&#34;&#34;&#34;

from functools import wraps

import numpy as np
import torch
import torch.nn.functional as F


def no_grad_dec(func):
    &#34;&#34;&#34;Decorate a function to avoid computing grads.&#34;&#34;&#34;

    @wraps(func)
    def f(*args, **kwargs):
        with torch.no_grad():
            return func(*args, **kwargs)

    return f


def log_sum_exp_trick(x, log_w=None, dim=-1, keepdim=False):
    &#34;&#34;&#34;Compute log(sum(w * exp(x), dim=dim, keepdim=keepdim)) safely.

    Uses the logsumexp trick for the computation of this quantity.
    &#34;&#34;&#34;
    if log_w is None:
        log_w = torch.zeros_like(x)

    x = x + log_w # this way we include w in the computation of M

    M = x.max(dim=dim, keepdim=True).values
    x = torch.log(torch.exp(x - M).sum(dim=dim, keepdim=True)) + M
    
    if not keepdim:
        x = x.squeeze(dim=dim)

    return x


def log_mean_exp_trick(x, dim=-1, keepdim=False):
    &#34;&#34;&#34;Computes log(mean(exp(x), dim=dim, keepdim=keepdim)) safely.

    Uses the logsumexp trick for the computation of this quantity.
    &#34;&#34;&#34;
    N = x.size(dim)

    return log_sum_exp_trick(x, dim=dim, keepdim=keepdim) - np.log(N)


softplus = lambda x, eps=1e-6, **kwargs: F.softplus(x, **kwargs) + eps

def softplus_inv(x, eps=1e-6, threshold=20.):
    &#34;&#34;&#34;Compute the softplus inverse.&#34;&#34;&#34;
    y = torch.zeros_like(x)

    idx = x &lt; threshold
    # We deliberately ignore eps to avoid -inf
    y[idx] = torch.log(torch.exp(x[idx]) - 1)
    y[~idx] = x[~idx]

    return y

logsigmoid = lambda x, alpha=1., **kwargs: -softplus(-alpha * x, **kwargs)


@no_grad_dec
def monotonic_increasing_bounded_bijective_search(
    f, x, *args, a=0., b=1., eps=1e-3, **kwargs
):
    &#34;&#34;&#34;Use bounded bijective search to solve x = f(u) for u.&#34;&#34;&#34;
    
    assert a &lt; b
    a, b = torch.ones_like(x) * float(a), torch.ones_like(x) * float(b)

    diff = eps * 2
    while diff &gt; eps:
        u = (a + b) / 2.
        fu = f(u, *args, **kwargs)

        lt = fu &lt; x
        a = torch.where(lt, u, a)
        b = torch.where(lt, b, u)

        diff = (fu - x).abs().mean()

    return u


@no_grad_dec
def monotonic_increasing_bijective_search(
    f, x, *args, a=-np.inf, b=np.inf, eps=1e-3, max_steps=100, **kwargs
):
    &#34;&#34;&#34;Use unbounded bijective search to solve x = f(u) for u.&#34;&#34;&#34;

    sig = lambda x, alpha: 1 / (1 + torch.exp(-alpha * x))
    logit = lambda x, alpha: -torch.log(1 / x - 1) / alpha

    assert a &lt; b
    a, b = torch.ones_like(x) * a, torch.ones_like(x) * b
    alpha = torch.ones_like(x)
    
    i_a, i_b = sig(a, alpha), sig(b, alpha)
    
    diff = eps * 2
    n_steps = 0
    while diff &gt;= eps:
        i_u = (i_a + i_b) / 2.
        u = logit(i_u, alpha)
        # Update alpha so that logit(i_u) has derivative 1
        # Get the original a, b, u
        a, b, u = logit(i_a, alpha), logit(i_b, alpha), logit(i_u, alpha)
        # Compute the new alpha (controlled so it doesn&#39;t go to inf)
        alpha = 1 / 1000 * (i_u - i_u ** 2)
        # alpha = alpha.clamp(.01, 10.)
        # Obtain the corresponding i_a, i_b, i_u
        i_a, i_b, i_u = sig(a, alpha), sig(b, alpha), sig(u, alpha)
        
        # Compute the image of u, and update bounds
        fu = f(u, *args, **kwargs)
            
        lt, gt = fu &lt; x, fu &gt; x
        i_a = torch.where(lt, i_u, i_a)
        i_b = torch.where(gt, i_u, i_b)
        
        # Can we stop early?
        diff = (fu - x).abs().max() # max to continue until we get the furthest point
        n_steps += 1

        if max_steps and n_steps &gt;= max_steps:
            break

    return u</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="flow.utils.log_mean_exp_trick"><code class="name flex">
<span>def <span class="ident">log_mean_exp_trick</span></span>(<span>x, dim=-1, keepdim=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes log(mean(exp(x), dim=dim, keepdim=keepdim)) safely.</p>
<p>Uses the logsumexp trick for the computation of this quantity.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_mean_exp_trick(x, dim=-1, keepdim=False):
    &#34;&#34;&#34;Computes log(mean(exp(x), dim=dim, keepdim=keepdim)) safely.

    Uses the logsumexp trick for the computation of this quantity.
    &#34;&#34;&#34;
    N = x.size(dim)

    return log_sum_exp_trick(x, dim=dim, keepdim=keepdim) - np.log(N)</code></pre>
</details>
</dd>
<dt id="flow.utils.log_sum_exp_trick"><code class="name flex">
<span>def <span class="ident">log_sum_exp_trick</span></span>(<span>x, log_w=None, dim=-1, keepdim=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute log(sum(w * exp(x), dim=dim, keepdim=keepdim)) safely.</p>
<p>Uses the logsumexp trick for the computation of this quantity.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_sum_exp_trick(x, log_w=None, dim=-1, keepdim=False):
    &#34;&#34;&#34;Compute log(sum(w * exp(x), dim=dim, keepdim=keepdim)) safely.

    Uses the logsumexp trick for the computation of this quantity.
    &#34;&#34;&#34;
    if log_w is None:
        log_w = torch.zeros_like(x)

    x = x + log_w # this way we include w in the computation of M

    M = x.max(dim=dim, keepdim=True).values
    x = torch.log(torch.exp(x - M).sum(dim=dim, keepdim=True)) + M
    
    if not keepdim:
        x = x.squeeze(dim=dim)

    return x</code></pre>
</details>
</dd>
<dt id="flow.utils.logsigmoid"><code class="name flex">
<span>def <span class="ident">logsigmoid</span></span>(<span>x, alpha=1.0, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">logsigmoid = lambda x, alpha=1., **kwargs: -softplus(-alpha * x, **kwargs)</code></pre>
</details>
</dd>
<dt id="flow.utils.monotonic_increasing_bijective_search"><code class="name flex">
<span>def <span class="ident">monotonic_increasing_bijective_search</span></span>(<span>f, x, *args, a=-inf, b=inf, eps=0.001, max_steps=100, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Use unbounded bijective search to solve x = f(u) for u.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@no_grad_dec
def monotonic_increasing_bijective_search(
    f, x, *args, a=-np.inf, b=np.inf, eps=1e-3, max_steps=100, **kwargs
):
    &#34;&#34;&#34;Use unbounded bijective search to solve x = f(u) for u.&#34;&#34;&#34;

    sig = lambda x, alpha: 1 / (1 + torch.exp(-alpha * x))
    logit = lambda x, alpha: -torch.log(1 / x - 1) / alpha

    assert a &lt; b
    a, b = torch.ones_like(x) * a, torch.ones_like(x) * b
    alpha = torch.ones_like(x)
    
    i_a, i_b = sig(a, alpha), sig(b, alpha)
    
    diff = eps * 2
    n_steps = 0
    while diff &gt;= eps:
        i_u = (i_a + i_b) / 2.
        u = logit(i_u, alpha)
        # Update alpha so that logit(i_u) has derivative 1
        # Get the original a, b, u
        a, b, u = logit(i_a, alpha), logit(i_b, alpha), logit(i_u, alpha)
        # Compute the new alpha (controlled so it doesn&#39;t go to inf)
        alpha = 1 / 1000 * (i_u - i_u ** 2)
        # alpha = alpha.clamp(.01, 10.)
        # Obtain the corresponding i_a, i_b, i_u
        i_a, i_b, i_u = sig(a, alpha), sig(b, alpha), sig(u, alpha)
        
        # Compute the image of u, and update bounds
        fu = f(u, *args, **kwargs)
            
        lt, gt = fu &lt; x, fu &gt; x
        i_a = torch.where(lt, i_u, i_a)
        i_b = torch.where(gt, i_u, i_b)
        
        # Can we stop early?
        diff = (fu - x).abs().max() # max to continue until we get the furthest point
        n_steps += 1

        if max_steps and n_steps &gt;= max_steps:
            break

    return u</code></pre>
</details>
</dd>
<dt id="flow.utils.monotonic_increasing_bounded_bijective_search"><code class="name flex">
<span>def <span class="ident">monotonic_increasing_bounded_bijective_search</span></span>(<span>f, x, *args, a=0.0, b=1.0, eps=0.001, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Use bounded bijective search to solve x = f(u) for u.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@no_grad_dec
def monotonic_increasing_bounded_bijective_search(
    f, x, *args, a=0., b=1., eps=1e-3, **kwargs
):
    &#34;&#34;&#34;Use bounded bijective search to solve x = f(u) for u.&#34;&#34;&#34;
    
    assert a &lt; b
    a, b = torch.ones_like(x) * float(a), torch.ones_like(x) * float(b)

    diff = eps * 2
    while diff &gt; eps:
        u = (a + b) / 2.
        fu = f(u, *args, **kwargs)

        lt = fu &lt; x
        a = torch.where(lt, u, a)
        b = torch.where(lt, b, u)

        diff = (fu - x).abs().mean()

    return u</code></pre>
</details>
</dd>
<dt id="flow.utils.no_grad_dec"><code class="name flex">
<span>def <span class="ident">no_grad_dec</span></span>(<span>func)</span>
</code></dt>
<dd>
<div class="desc"><p>Decorate a function to avoid computing grads.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def no_grad_dec(func):
    &#34;&#34;&#34;Decorate a function to avoid computing grads.&#34;&#34;&#34;

    @wraps(func)
    def f(*args, **kwargs):
        with torch.no_grad():
            return func(*args, **kwargs)

    return f</code></pre>
</details>
</dd>
<dt id="flow.utils.softplus"><code class="name flex">
<span>def <span class="ident">softplus</span></span>(<span>x, eps=1e-06, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">softplus = lambda x, eps=1e-6, **kwargs: F.softplus(x, **kwargs) + eps</code></pre>
</details>
</dd>
<dt id="flow.utils.softplus_inv"><code class="name flex">
<span>def <span class="ident">softplus_inv</span></span>(<span>x, eps=1e-06, threshold=20.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the softplus inverse.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def softplus_inv(x, eps=1e-6, threshold=20.):
    &#34;&#34;&#34;Compute the softplus inverse.&#34;&#34;&#34;
    y = torch.zeros_like(x)

    idx = x &lt; threshold
    # We deliberately ignore eps to avoid -inf
    y[idx] = torch.log(torch.exp(x[idx]) - 1)
    y[~idx] = x[~idx]

    return y</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flow" href="index.html">flow</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="flow.utils.log_mean_exp_trick" href="#flow.utils.log_mean_exp_trick">log_mean_exp_trick</a></code></li>
<li><code><a title="flow.utils.log_sum_exp_trick" href="#flow.utils.log_sum_exp_trick">log_sum_exp_trick</a></code></li>
<li><code><a title="flow.utils.logsigmoid" href="#flow.utils.logsigmoid">logsigmoid</a></code></li>
<li><code><a title="flow.utils.monotonic_increasing_bijective_search" href="#flow.utils.monotonic_increasing_bijective_search">monotonic_increasing_bijective_search</a></code></li>
<li><code><a title="flow.utils.monotonic_increasing_bounded_bijective_search" href="#flow.utils.monotonic_increasing_bounded_bijective_search">monotonic_increasing_bounded_bijective_search</a></code></li>
<li><code><a title="flow.utils.no_grad_dec" href="#flow.utils.no_grad_dec">no_grad_dec</a></code></li>
<li><code><a title="flow.utils.softplus" href="#flow.utils.softplus">softplus</a></code></li>
<li><code><a title="flow.utils.softplus_inv" href="#flow.utils.softplus_inv">softplus_inv</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>